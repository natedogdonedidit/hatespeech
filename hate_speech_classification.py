# -*- coding: utf-8 -*-
"""Hate Speech Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19EpysEFQDlyMi7zhdc9-8apq0v9l3Npg
"""

#Hate Speech Classification


#pip install spacy

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Embedding, Dropout 
from tensorflow.keras.layers import Conv1D, MaxPool1D, GlobalMaxPooling1D

import numpy as np
import pandas as pd
import re
from sklearn.model_selection import train_test_split
import preprocess_kgptalkie as ps

#dataset balancing
df = pd.read_csv('https://raw.githubusercontent.com/laxmimerit/hate_speech_dataset/master/data.csv', index_col=0) #why index col = 0? because we dont want the first column
df.head()

#0 - hate speech 1 - offensive language 2 - neither hate nor offensive

df['class'].value_counts()

vc = df['class'].value_counts()
index = list(vc.index)
count = min(vc.values) #going to return the min number in our values

count, index

# WE ARE BALANCING THE DATA SO WE HAVE EQUAL NUMBER OF ROWS PER CLASSSSSS
df_bal = pd.DataFrame()
for i in index: #what was the index? 1,2,0. we are iterating for all the class values 012
  temp = df[df['class']==i].sample(count)
  df_bal = df_bal.append(temp, ignore_index = True) #we are appending

df = df_bal.copy()

df[df['class']==2]

df['class'].value_counts() 
# EQUAL NUMBER OF SAMPLES FOR ALL CLASSES

# DATA PREPROCESSING
def get_clean(x):
    x = str(x).lower().replace('\\', '').replace('_', ' ')
    x = ps.cont_exp(x)
    x = ps.remove_emails(x)
    x = ps.remove_urls(x)
    x = ps.remove_html_tags(x)
    x = ps.remove_rt(x)
    x = ps.remove_accented_chars(x)
    x = ps.remove_special_chars(x)
    x = re.sub("(.)\\1{2,}", "\\1", x)
    return x

x = 'iiii loooovvveee ___you#$'

get_clean(x)

df['tweet'] = df['tweet'].apply(lambda x: get_clean(x))

df.head()

#NOW WE ARE READY TO PUT OUR DATA THROUGH OUR ML TOKENIZER

# HAVE TO CONVERT TEXT DATA TO FORM OF LIST
text = df['tweet'].tolist()

text[:3]

token = Tokenizer() #create tokenizer object
token.fit_on_texts(text)
#tokens are now stored within the token variable

token.word_counts
#this is the dictionary with all the words and number of occurences

len(token.word_counts)
#number of words within the text data

print(token.index_word)
#words indexed in sequence and how they are indexed. 'a' is placed in 1 loction, and so on.

x = ['i love you']

token.texts_to_sequences(x)
#this shows how x corresponds to the values in our indexed dictionary. 'i (2) love(75) you (4)'

vocab_size = len(token.word_counts) + 1

#now lets put all our text data and encode it all
encoded_text = token.texts_to_sequences(text)

print(encoded_text)
#this is encoded in form of lists. actually, a list of lists. first list is all the text data, the lists within that lists are all of the individual sentences

max_length = 120
X = pad_sequences(encoded_text, maxlen=max_length, padding = 'post')

print(X)

X.shape
#now we have 4290 rows and the vector size is normalized as a max length of 120?

# NOW LETS BUILD OUR MODEL

from keras.utils import np_utils
from tensorflow.keras.optimizers import Adam

y = df['class']
#now we need to convert to encoded/categorical data

y
#at start

y = np_utils.to_categorical(df['class'])
#this is how we get y, we are converting y (which reps our classes) into encoded categories

print(y)
#now we have our y we can use

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)

X_train.shape, X_test.shape

vec_size = 300 #internally, DL keras makes vector representaions for all the tokens. 
model = Sequential()
model.add(Embedding(vocab_size, vec_size, input_length=max_length)) #embedding takes input dimension, and that is vocab_size for us and how many output dim you want (vector representation), we want 300, then waht is max input length? we defined as 120
# now we need to add our CNN model
model.add(Conv1D(32, 2, activation='relu')) #we need to determine number of filters for our first layer, which is 32, then filter size, which is 2, then activiation function which is relu
model.add(MaxPool1D(2)) #this means it will see the text data, then it will select the max of those two after conv layer
model.add(Dropout(0.2))
#above we have only added one layer of ConvNN, because we have a smaller dataset
# below we are adding a fully connected dense layer with 32 units?
model.add(Dense(32, activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(3, activation='softmax')) #this is output layer, tricky, we have 3 classes, its multiclass, so we use 3 units here, using activation function sfotmax bc multi

model.compile(optimizer=Adam(learning_rate=0.001), loss = 'categorical_crossentropy', metrics = ['accuracy']) 
#for loss using cat cross, earlier we used binary, but we only had 2 classes, now we have more, need to use cat cross now
#metrics is for what we are training our model, what we are optimizing for, which is accuracy
model.fit(X_train, y_train, epochs = 2, validation_data=(X_test, y_test), shuffle = True)
#validation is just us saying hey how are we gonna validate this? and we are going to do so using our test data
#make sure you've imported the proper packages so we can use this model we built above

#first epoch we are getting 80% accuracy, second is 80 as well pretty good accuracy for our limited data set

# MODEL TESTINGGGG

from sklearn.metrics import classification_report, confusion_matrix


#these are the methods we gonna use to test our model
y_pred = np.argmax(model.predict(X_test), axis=-1)

y_test
#problem with y test is that its in form of categorical data, we need to convert it to the same format we have y_pred

#CONVERT y_test!
np.argmax(y_test, axis=-1)
#NOTICE WE ARE USING THE SAME METHOD (np.argmax) TO CONVERT y_test to same format as y_pred

#NOW LETS PLOT IT

#there are some miss classifications in our matrix, but its still pretty accurate.

print(classification_report(np.argmax(y_test, axis=-1), y_pred))

#testing with custom data

x = 'fuck off bro'

#whatever preprocessing we have done for our data we have to do for our testing as well. we have to do it here as well
def get_encoded(x):
  x = get_clean(x)
  x = token.texts_to_sequences([x]) #x data in form of a list
  x = pad_sequences(x, maxlen=max_length, padding = 'post') #this is exactly what we used during the prep of our training data
  return x

get_encoded(x)
#in this sequence we get what we wanted. it has 3 words, and all others are zero / padded with zero?
#now we have the data, we can pass it through the model

np.argmax(model.predict(get_encoded(x)), axis=-1)
#we get 2, which classifies this as neither hate speech or offensive language
#we are going to change it, but this example x was = 'hey dude whass up'
#hey bitch wassup changes over to 1, which is offensive language
#we changed x to 'hey nigger wassup' and it classifies it as 0

# in order to save the model, we have to save the model and the token, and we have to remember what we used for max_length and padding, but
#store only token and model.

model.save('model_hate_speech.h5')

import pickle

pickle.dump(token, open('token.pkl', 'wb'))

#whenever you reload it:
# from keras.models import load_model







